{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Python Standard Library Modules\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# External Libraries\n",
    "from gluonts.dataset import common\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.model import deepar\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from hyperopt import fmin, hp, tpe, STATUS_OK, STATUS_FAIL, Trials\n",
    "from hyperopt.pyll import scope\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mx.random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "prediction_length = 30\n",
    "# validation_length = 30\n",
    "# if validation_length:\n",
    "#     prediction_length = prediction_length + validation_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAHOO Finance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def covert_yahoo_series_dir(path: str, prediction_length: int) -> list:\n",
    "    \"\"\"Clean and load all coin histories in the Yahoo Finance folder\n",
    "\n",
    "    Params:\n",
    "        path: folder full of historical crypto coins timeseries data\n",
    "        prediction_length: length on which to predict\n",
    "    Returns:\n",
    "        List of Gluon-compatible dicts from the coin data\n",
    "    \"\"\"\n",
    "    gluon_list = []\n",
    "    for file in os.listdir(path):\n",
    "        coin_gluon_dict = dict()\n",
    "        file_path = path + file\n",
    "        coin = pd.read_csv(file_path)\n",
    "        coin[\"Date\"] = pd.to_datetime(coin[\"Date\"])\n",
    "        coin.set_index(\"Date\", inplace=True)\n",
    "        if len(coin) < 100:\n",
    "            continue\n",
    "        coin = coin.asfreq(\"D\")\n",
    "        total_na_vals = coin.isna().sum()[0]\n",
    "        if (total_na_vals / len(coin)) > 0.25:\n",
    "            continue\n",
    "        # Get values for ListDatasets\n",
    "        coin_closes = coin[\"Close\"]\n",
    "        coin_closes.index = pd.DatetimeIndex(coin_closes.index)\n",
    "        coin_closes = coin_closes.asfreq(\"D\")\n",
    "#         coin_closes.fillna(method='bfill', inplace=True)\n",
    "#         coin_closes.dropna(inplace=True)\n",
    "        start = coin_closes.index[0]\n",
    "\n",
    "        coin_gluon_dict[\"test\"] = {\n",
    "            \"start\": start,\n",
    "            \"target\": coin_closes,\n",
    "            \"name\": file,\n",
    "        }\n",
    "\n",
    "#         coin_gluon_dict[\"validation\"] = {\n",
    "#             \"start\": start,\n",
    "#             \"target\": coin_closes[:-30],\n",
    "#             \"name\": file,\n",
    "#         }\n",
    "\n",
    "        coin_gluon_dict[\"train\"] = {\n",
    "            \"start\": start,\n",
    "            \"target\": coin_closes[:-prediction_length],\n",
    "            \"name\": file,\n",
    "        }\n",
    "\n",
    "        gluon_list.append(coin_gluon_dict)\n",
    "\n",
    "    return gluon_list\n",
    "\n",
    "\n",
    "gluon_list = covert_yahoo_series_dir(\"../data/historical_yahoo_crypto/\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce List Order BTC, ETH, Cardano at the beginning\n",
    "new_list = []\n",
    "btc_gluon_dict = None\n",
    "eth_gluon_dict = None\n",
    "card_gluon_dict = None\n",
    "for index, value in enumerate(gluon_list):\n",
    "    if value['test']['name'] == \"BTC-USD.csv\":\n",
    "        btc_gluon_dict = value\n",
    "    elif value['test']['name'] == \"ETH-USD.csv\":\n",
    "        eth_gluon_dict = value\n",
    "    elif value['test']['name'] == \"ADA-USD.csv\":\n",
    "        card_gluon_dict = value\n",
    "    else:\n",
    "        new_list.append(value)\n",
    "\n",
    "\n",
    "new_list.insert(0, card_gluon_dict)   \n",
    "new_list.insert(0, eth_gluon_dict)\n",
    "new_list.insert(0, btc_gluon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"always\")\n",
    "\n",
    "import mxnet as mx\n",
    "mx.random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "test_data = common.ListDataset(\n",
    "    [\n",
    "    ],\n",
    "    freq=\"D\",\n",
    ")\n",
    "\n",
    "# validation_data = common.ListDataset(\n",
    "#     [\n",
    "#     ],\n",
    "#     freq=\"D\",\n",
    "# )\n",
    "\n",
    "train_data = common.ListDataset(\n",
    "    [\n",
    "    ],\n",
    "    freq=\"D\",\n",
    ")\n",
    "\n",
    "\n",
    "for coin_gluon_dict in new_list:\n",
    "    test_data.list_data.append(coin_gluon_dict['test'])\n",
    "#     validation_data.list_data.append(coin_gluon_dict['validation'])\n",
    "    train_data.list_data.append(coin_gluon_dict['train'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPEROPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRYING: {'batch_size': 63, 'cell_type': 'lstm', 'context_length': 33, 'epochs': 3, 'learning_rate': 0.04735, 'num_cells': 71, 'num_layers': 1}\n",
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/core/component.py:345: DeprecationWarning: batch_size argument is deprecated\n",
      "  return init(self, **all_args)\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:158: DeprecationWarning: Trainer argument \"learning_rate_decay_factor\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:166: DeprecationWarning: Trainer argument \"patience\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:172: DeprecationWarning: Trainer argument \"minimum_learning_rate\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      " 56%|#####6    | 28/50 [00:03<00:02,  8.76it/s, epoch=1/3, avg_epoch_loss=0.0553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:                                                 \n",
      "TRYING: {'batch_size': 248, 'cell_type': 'lstm', 'context_length': 87, 'epochs': 9, 'learning_rate': 0.0175, 'num_cells': 59, 'num_layers': 4}\n",
      "  1%|          | 1/100 [00:03<05:22,  3.26s/trial, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/core/component.py:345: DeprecationWarning: batch_size argument is deprecated\n",
      "  return init(self, **all_args)\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:158: DeprecationWarning: Trainer argument \"learning_rate_decay_factor\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:166: DeprecationWarning: Trainer argument \"patience\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:172: DeprecationWarning: Trainer argument \"minimum_learning_rate\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      " 28%|##8       | 14/50 [00:10<00:26,  1.36it/s, epoch=1/9, avg_epoch_loss=2.05]\n",
      "\u001b[A\n",
      " 50%|#####     | 25/50 [00:18<00:18,  1.38it/s, epoch=1/9, avg_epoch_loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:                                                         \n",
      "TRYING: {'batch_size': 67, 'cell_type': 'lstm', 'context_length': 154, 'epochs': 4, 'learning_rate': 0.08695, 'num_cells': 74, 'num_layers': 3}\n",
      "  2%|▏         | 2/100 [00:21<19:37, 12.02s/trial, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/core/component.py:345: DeprecationWarning: batch_size argument is deprecated\n",
      "  return init(self, **all_args)\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:158: DeprecationWarning: Trainer argument \"learning_rate_decay_factor\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:166: DeprecationWarning: Trainer argument \"patience\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:172: DeprecationWarning: Trainer argument \"minimum_learning_rate\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      " 16%|#6        | 8/50 [00:08<00:45,  1.08s/it, epoch=1/4, avg_epoch_loss=5.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:                                                         \n",
      "TRYING: {'batch_size': 214, 'cell_type': 'lstm', 'context_length': 88, 'epochs': 8, 'learning_rate': 0.06035, 'num_cells': 81, 'num_layers': 3}\n",
      "  3%|▎         | 3/100 [00:30<16:56, 10.48s/trial, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/core/component.py:345: DeprecationWarning: batch_size argument is deprecated\n",
      "  return init(self, **all_args)\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:158: DeprecationWarning: Trainer argument \"learning_rate_decay_factor\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:166: DeprecationWarning: Trainer argument \"patience\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:172: DeprecationWarning: Trainer argument \"minimum_learning_rate\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      " 36%|###6      | 18/50 [00:10<00:18,  1.72it/s, epoch=1/8, avg_epoch_loss=3.47]\n",
      "\u001b[A\n"
     ]
    }
   ],
   "source": [
    "mx.random.seed(0)\n",
    "\n",
    "# search_space = {\n",
    "#     'epochs': scope.int(hp.quniform('epochs', 1, 20, q=1)),\n",
    "#     'num_layers': scope.int(hp.quniform('num_layers', 1, 8, q=1)),\n",
    "#     'num_cells': scope.int(hp.quniform('num_cells', 30, 100, q=1)),\n",
    "#     'cell_type': hp.choice('cell_type', ['lstm', 'gru']),\n",
    "#     'batch_size': scope.int(hp.quniform('batch_size', 16, 256, q=1)),\n",
    "#     'learning_rate': hp.quniform('learning_rate', 5e-5, 1e-1, 0.00005),\n",
    "#     'context_length': scope.int(hp.quniform('context_length', 1, 200, q=1)),\n",
    "# }\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    'epochs': scope.int(hp.quniform('epochs', 1, 10, q=1)),\n",
    "    'num_layers': scope.int(hp.quniform('num_layers', 1, 4, q=1)),\n",
    "    'num_cells': scope.int(hp.quniform('num_cells', 30, 100, q=1)),\n",
    "    'cell_type': hp.choice('cell_type', ['lstm', 'gru']),\n",
    "    'batch_size': scope.int(hp.quniform('batch_size', 40, 256, q=1)),\n",
    "    'learning_rate': hp.quniform('learning_rate', 5e-5, 1e-1, 0.00005),\n",
    "    'context_length': scope.int(hp.quniform('context_length', 1, 200, q=1)),\n",
    "}\n",
    "\n",
    "def global_objective(params):\n",
    "    print(f\"TRYING: {params}\")\n",
    "    try:\n",
    "        deepar_params = {k: v for k,v in params.items() if k not in ('epochs', 'batch_size', 'learning_rate')}\n",
    "        epochs = params['epochs']\n",
    "        batch_size = params['batch_size']\n",
    "        learning_rate = params['learning_rate']\n",
    "        cell_type = params['cell_type']\n",
    "        trainer = Trainer(\n",
    "                          epochs=epochs, \n",
    "                          batch_size=batch_size, \n",
    "                          learning_rate=learning_rate,\n",
    "                          clip_gradient=2.5, # to avoid weird endless NaN bug\n",
    "                          hybridize=False, # to avoid weird endless NaN bug\n",
    "                          )\n",
    "        estimator = deepar.DeepAREstimator(\n",
    "            freq=\"1D\", \n",
    "            prediction_length=prediction_length, \n",
    "            trainer=trainer,\n",
    "            **deepar_params\n",
    "        )\n",
    "        predictor = estimator.train(training_data=train_data)\n",
    "        global_loss = 0\n",
    "        predictions = predictor.predict(train_data.list_data)\n",
    "        for index, value in enumerate(range(len(new_list))):\n",
    "            prediction = next(predictions)\n",
    "            name = test_data.list_data[index]['name']\n",
    "            # Skip graphs with absurd loss, stablecoins etc.\n",
    "            if name in {\"USDT-USD.csv\", \"CCXX-USD.csv\", \"TUSD-USD.csv\"}:\n",
    "                continue\n",
    "            full_actual = test_data.list_data[index]['target']\n",
    "            actual = full_actual[-30:]\n",
    "            preds = pd.Series(prediction.mean)\n",
    "            preds.index = actual.index\n",
    "\n",
    "            # SCALING\n",
    "            scaler = MinMaxScaler()\n",
    "            scaled_actual = np.array(actual)\n",
    "            scaler.fit([scaled_actual])\n",
    "            scaled_actual = scaler.fit_transform(np.array(scaled_actual[:, np.newaxis]))\n",
    "            scaled_preds = scaler.transform([preds])\n",
    "            scaled_preds = scaled_preds.reshape(-1, 1)\n",
    "            mse = mean_squared_error(scaled_actual, scaled_preds)\n",
    "            global_loss += mse\n",
    "\n",
    "        return {'loss': global_loss, 'status': STATUS_OK}\n",
    "    \n",
    "    except BaseException as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return {'status': STATUS_FAIL}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    global_objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:158: DeprecationWarning: Trainer argument \"learning_rate_decay_factor\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:166: DeprecationWarning: Trainer argument \"patience\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:172: DeprecationWarning: Trainer argument \"minimum_learning_rate\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.95it/s, epoch=1/2, avg_epoch_loss=0.63]\n",
      "100%|██████████| 50/50 [00:03<00:00, 15.67it/s, epoch=2/2, avg_epoch_loss=-.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_loss is 47.499963074804114\n",
      "average global_loss is 0.4611646900466419\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(epochs=2, learning_rate=0.0001)\n",
    "estimator = deepar.DeepAREstimator(\n",
    "        freq=\"D\", \n",
    "#         num_cells=150,\n",
    "#         num_layers=3,\n",
    "        prediction_length=prediction_length, \n",
    "        trainer=trainer,\n",
    "#         context_length=100,\n",
    "#         use_feat_dynamic_real=True,\n",
    "    )\n",
    "\n",
    "predictor = estimator.train(\n",
    "    training_data=train_data,\n",
    "#     validation_data=validation_data\n",
    ")\n",
    "\n",
    "\n",
    "mx.random.seed(0)\n",
    "global_loss = 0\n",
    "predictions = predictor.predict(train_data.list_data)\n",
    "for index, value in enumerate(range(len(gluon_list))):\n",
    "    prediction = next(predictions)\n",
    "    name = test_data.list_data[index]['name']\n",
    "    # Skip graphs with absurd loss, stablecoins etc.\n",
    "    if name in {\"USDT-USD.csv\", \"CCXX-USD.csv\", \"TUSD-USD.csv\"}:\n",
    "        continue\n",
    "#     print(name)\n",
    "    full_actual = test_data.list_data[index]['target']\n",
    "    actual = full_actual[-30:]\n",
    "    preds = pd.Series(prediction.mean)\n",
    "    preds.index = actual.index\n",
    "    # PLOT ALL CRYPTO PREDICTIONS\n",
    "#     plt.figure()\n",
    "#     preds.plot(legend=True, label=f\"{name} PREDICTED\")\n",
    "#     actual.plot(legend=True, label=f\"{name} ACTUAL\")\n",
    "#     plt.show()\n",
    "\n",
    "    # SCALING\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_actual = np.array(actual)\n",
    "    scaler.fit([scaled_actual])\n",
    "    scaled_actual = scaler.fit_transform(np.array(scaled_actual[:, np.newaxis]))\n",
    "    scaled_preds = scaler.transform([preds])\n",
    "    scaled_preds = scaled_preds.reshape(-1, 1)\n",
    "    mse = mean_squared_error(scaled_actual, scaled_preds)\n",
    "#     print(f\"mse: {mse}\")\n",
    "    global_loss += mse\n",
    "    \n",
    "print(f\"global_loss is {global_loss}\")\n",
    "print(f\"average global_loss is {global_loss / len(gluon_list)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "21 with absolutely nothing\n",
    "Doesn't work with dropna\n",
    "25 with fillna\n",
    "\n",
    "\n",
    "122.9 BASE 20 epochs, learning_rate=0.01, num_cells=50\n",
    "119 BASE 10 epochs, learning_rate=0.01, num_cells=100, num_layers=3\n",
    "91.1 BASE 10 epochs, learning_rate=0.01, num_cells=100\n",
    "83 BASE 20 epochs, learning_rate=0.003\n",
    "78.4 BASE 10 epochs, context_length=60\n",
    "78 150, 3, 0.001, 1 epochs\n",
    "67.07 BASE 10 epochs, context_length=100\n",
    "67.07 BASE 10 epochs, context_length=100, batch_size=64\n",
    "56.76 BASE 20 epochs, learning_rate=0.01\n",
    "55.4 BASE num_cells=150, num_layers=3, 10 epochs, context_length=100, batch_size=64 \n",
    "45.82 BASE 20 epochs, num_cells=60, num_layers=3\n",
    "40.32 BASE 10 epochs\n",
    "37 BASE 1 epochs\n",
    "36.6 BASE 20 epochs, num_cells=50, num_layers=3, learning_rate=0.0001\n",
    "34.75 BASE 10 epochs, num_cells=60\n",
    "33.68 BASE 3 epochs\n",
    "32.56 BASE 20 epochs, num_cells=60\n",
    "30.9 BASE 20 epochs, num_cells=60, num_layers=3, learning_rate=0.0001\n",
    "29.57 BASE 10 epochs, num_cells=50\n",
    "26.7 BASE 2 epochs\n",
    "23.4 BASE 5 epochs\n",
    "22.86 BASE 20 epochs, learning_rate=0.0002\n",
    "21.95 BASE 50 epochs\n",
    "21.95 BASE 30 epochs\n",
    "21.52 BASE 20 epochs, learning_rate=0.0001\n",
    "21.1 BASE 20 epochs, learning_rate=0.0005\n",
    "21 150, 3, 0.001, 50 epochs\n",
    "20.9 BASE 20 epochs\n",
    "\n",
    "\n",
    "32 num_cells=150, num_layers=3, 0.0001, 40 epochs\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
