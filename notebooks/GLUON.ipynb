{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Python Standard Library Modules\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# External Libraries\n",
    "from gluonts.dataset import common\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.model import deepar\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from hyperopt import fmin, hp, tpe, STATUS_OK, STATUS_FAIL, Trials\n",
    "from hyperopt.pyll import scope\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mx.random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "prediction_length = 30\n",
    "# validation_length = 30\n",
    "# if validation_length:\n",
    "#     prediction_length = prediction_length + validation_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_evaluation_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAHOO Finance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def covert_yahoo_series_dir(path: str, prediction_length: int) -> list:\n",
    "    \"\"\"Clean and load all coin histories in the Yahoo Finance folder\n",
    "\n",
    "    Params:\n",
    "        path: folder full of historical crypto coins timeseries data\n",
    "        prediction_length: length on which to predict\n",
    "    Returns:\n",
    "        List of Gluon-compatible dicts from the coin data\n",
    "    \"\"\"\n",
    "    gluon_list = []\n",
    "    for file in os.listdir(path):\n",
    "        coin_gluon_dict = dict()\n",
    "        file_path = path + file\n",
    "        coin = pd.read_csv(file_path)\n",
    "        coin[\"Date\"] = pd.to_datetime(coin[\"Date\"])\n",
    "        coin.set_index(\"Date\", inplace=True)\n",
    "        if len(coin) < 100:\n",
    "            continue\n",
    "        coin = coin.asfreq(\"D\")\n",
    "        total_na_vals = coin.isna().sum()[0]\n",
    "        if (total_na_vals / len(coin)) > 0.25:\n",
    "            continue\n",
    "        # Get values for ListDatasets\n",
    "        coin_closes = coin[\"Close\"]\n",
    "        coin_closes.index = pd.DatetimeIndex(coin_closes.index)\n",
    "        coin_closes = coin_closes.asfreq(\"D\")\n",
    "#         coin_closes.fillna(method='bfill', inplace=True)\n",
    "#         coin_closes.dropna(inplace=True)\n",
    "        start = coin_closes.index[0]\n",
    "\n",
    "        coin_gluon_dict[\"test\"] = {\n",
    "            \"start\": start,\n",
    "            \"target\": coin_closes,\n",
    "            \"name\": file,\n",
    "        }\n",
    "\n",
    "#         coin_gluon_dict[\"validation\"] = {\n",
    "#             \"start\": start,\n",
    "#             \"target\": coin_closes[:-30],\n",
    "#             \"name\": file,\n",
    "#         }\n",
    "\n",
    "        coin_gluon_dict[\"train\"] = {\n",
    "            \"start\": start,\n",
    "            \"target\": coin_closes[:-prediction_length],\n",
    "            \"name\": file,\n",
    "        }\n",
    "\n",
    "        gluon_list.append(coin_gluon_dict)\n",
    "\n",
    "    return gluon_list\n",
    "\n",
    "\n",
    "gluon_list = covert_yahoo_series_dir(\"../data/misc_data/historical_yahoo_crypto/\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce List Order BTC, ETH, Cardano at the beginning\n",
    "new_list = []\n",
    "btc_gluon_dict = None\n",
    "eth_gluon_dict = None\n",
    "card_gluon_dict = None\n",
    "for index, value in enumerate(gluon_list):\n",
    "    if value['test']['name'] == \"BTC-USD.csv\":\n",
    "        btc_gluon_dict = value\n",
    "    elif value['test']['name'] == \"ETH-USD.csv\":\n",
    "        eth_gluon_dict = value\n",
    "    elif value['test']['name'] == \"ADA-USD.csv\":\n",
    "        card_gluon_dict = value\n",
    "    else:\n",
    "        new_list.append(value)\n",
    "\n",
    "\n",
    "new_list.insert(0, card_gluon_dict)   \n",
    "new_list.insert(0, eth_gluon_dict)\n",
    "new_list.insert(0, btc_gluon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"always\")\n",
    "\n",
    "import mxnet as mx\n",
    "mx.random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "test_data = common.ListDataset(\n",
    "    [\n",
    "    ],\n",
    "    freq=\"D\",\n",
    ")\n",
    "\n",
    "# validation_data = common.ListDataset(\n",
    "#     [\n",
    "#     ],\n",
    "#     freq=\"D\",\n",
    "# )\n",
    "\n",
    "train_data = common.ListDataset(\n",
    "    [\n",
    "    ],\n",
    "    freq=\"D\",\n",
    ")\n",
    "\n",
    "\n",
    "for coin_gluon_dict in new_list:\n",
    "    test_data.list_data.append(coin_gluon_dict['test'])\n",
    "#     validation_data.list_data.append(coin_gluon_dict['validation'])\n",
    "    train_data.list_data.append(coin_gluon_dict['train'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPEROPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRYING: {'batch_size': 63, 'cell_type': 'lstm', 'context_length': 33, 'epochs': 3, 'learning_rate': 0.04735, 'num_cells': 71, 'num_layers': 1}\n",
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/core/component.py:345: DeprecationWarning: batch_size argument is deprecated\n",
      "  return init(self, **all_args)\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:158: DeprecationWarning: Trainer argument \"learning_rate_decay_factor\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:166: DeprecationWarning: Trainer argument \"patience\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:172: DeprecationWarning: Trainer argument \"minimum_learning_rate\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      " 56%|#####6    | 28/50 [00:03<00:02,  8.76it/s, epoch=1/3, avg_epoch_loss=0.0553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:                                                 \n",
      "TRYING: {'batch_size': 248, 'cell_type': 'lstm', 'context_length': 87, 'epochs': 9, 'learning_rate': 0.0175, 'num_cells': 59, 'num_layers': 4}\n",
      "  1%|          | 1/100 [00:03<05:22,  3.26s/trial, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/core/component.py:345: DeprecationWarning: batch_size argument is deprecated\n",
      "  return init(self, **all_args)\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:158: DeprecationWarning: Trainer argument \"learning_rate_decay_factor\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:166: DeprecationWarning: Trainer argument \"patience\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:172: DeprecationWarning: Trainer argument \"minimum_learning_rate\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      " 28%|##8       | 14/50 [00:10<00:26,  1.36it/s, epoch=1/9, avg_epoch_loss=2.05]\n",
      "\u001b[A\n",
      " 50%|#####     | 25/50 [00:18<00:18,  1.38it/s, epoch=1/9, avg_epoch_loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:                                                         \n",
      "TRYING: {'batch_size': 67, 'cell_type': 'lstm', 'context_length': 154, 'epochs': 4, 'learning_rate': 0.08695, 'num_cells': 74, 'num_layers': 3}\n",
      "  2%|▏         | 2/100 [00:21<19:37, 12.02s/trial, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/core/component.py:345: DeprecationWarning: batch_size argument is deprecated\n",
      "  return init(self, **all_args)\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:158: DeprecationWarning: Trainer argument \"learning_rate_decay_factor\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:166: DeprecationWarning: Trainer argument \"patience\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:172: DeprecationWarning: Trainer argument \"minimum_learning_rate\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      " 16%|#6        | 8/50 [00:08<00:45,  1.08s/it, epoch=1/4, avg_epoch_loss=5.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:                                                         \n",
      "TRYING: {'batch_size': 214, 'cell_type': 'lstm', 'context_length': 88, 'epochs': 8, 'learning_rate': 0.06035, 'num_cells': 81, 'num_layers': 3}\n",
      "  3%|▎         | 3/100 [00:30<16:56, 10.48s/trial, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/core/component.py:345: DeprecationWarning: batch_size argument is deprecated\n",
      "  return init(self, **all_args)\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:158: DeprecationWarning: Trainer argument \"learning_rate_decay_factor\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:166: DeprecationWarning: Trainer argument \"patience\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:172: DeprecationWarning: Trainer argument \"minimum_learning_rate\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      " 36%|###6      | 18/50 [00:10<00:18,  1.72it/s, epoch=1/8, avg_epoch_loss=3.47]\n",
      "\u001b[A\n"
     ]
    }
   ],
   "source": [
    "mx.random.seed(0)\n",
    "\n",
    "# search_space = {\n",
    "#     'epochs': scope.int(hp.quniform('epochs', 1, 20, q=1)),\n",
    "#     'num_layers': scope.int(hp.quniform('num_layers', 1, 8, q=1)),\n",
    "#     'num_cells': scope.int(hp.quniform('num_cells', 30, 100, q=1)),\n",
    "#     'cell_type': hp.choice('cell_type', ['lstm', 'gru']),\n",
    "#     'batch_size': scope.int(hp.quniform('batch_size', 16, 256, q=1)),\n",
    "#     'learning_rate': hp.quniform('learning_rate', 5e-5, 1e-1, 0.00005),\n",
    "#     'context_length': scope.int(hp.quniform('context_length', 1, 200, q=1)),\n",
    "# }\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    'epochs': scope.int(hp.quniform('epochs', 1, 10, q=1)),\n",
    "    'num_layers': scope.int(hp.quniform('num_layers', 1, 4, q=1)),\n",
    "    'num_cells': scope.int(hp.quniform('num_cells', 30, 100, q=1)),\n",
    "    'cell_type': hp.choice('cell_type', ['lstm', 'gru']),\n",
    "    'batch_size': scope.int(hp.quniform('batch_size', 40, 256, q=1)),\n",
    "    'learning_rate': hp.quniform('learning_rate', 5e-5, 1e-1, 0.00005),\n",
    "    'context_length': scope.int(hp.quniform('context_length', 1, 200, q=1)),\n",
    "}\n",
    "\n",
    "def global_objective(params):\n",
    "    print(f\"TRYING: {params}\")\n",
    "    try:\n",
    "        deepar_params = {k: v for k,v in params.items() if k not in ('epochs', 'batch_size', 'learning_rate')}\n",
    "        epochs = params['epochs']\n",
    "        batch_size = params['batch_size']\n",
    "        learning_rate = params['learning_rate']\n",
    "        cell_type = params['cell_type']\n",
    "        trainer = Trainer(\n",
    "                          epochs=epochs, \n",
    "                          batch_size=batch_size, \n",
    "                          num_batches_per_epoch:\n",
    "                          learning_rate=learning_rate,\n",
    "                          clip_gradient=2.5, # to avoid weird endless NaN bug\n",
    "                          hybridize=False, # to avoid weird endless NaN bug\n",
    "                          )\n",
    "        estimator = deepar.DeepAREstimator(\n",
    "            freq=\"1D\", \n",
    "            prediction_length=prediction_length, \n",
    "            trainer=trainer,\n",
    "            **deepar_params\n",
    "        )\n",
    "        predictor = estimator.train(training_data=train_data)\n",
    "        global_loss = 0\n",
    "        predictions = predictor.predict(train_data.list_data)\n",
    "        for index, value in enumerate(range(len(new_list))):\n",
    "            prediction = next(predictions)\n",
    "            name = test_data.list_data[index]['name']\n",
    "            # Skip graphs with absurd loss, stablecoins etc.\n",
    "            if name in {\"USDT-USD.csv\", \"CCXX-USD.csv\", \"TUSD-USD.csv\"}:\n",
    "                continue\n",
    "            full_actual = test_data.list_data[index]['target']\n",
    "            actual = full_actual[-30:]\n",
    "            preds = pd.Series(prediction.mean)\n",
    "            preds.index = actual.index\n",
    "\n",
    "            # SCALING\n",
    "            scaler = MinMaxScaler()\n",
    "            scaled_actual = np.array(actual)\n",
    "            scaler.fit([scaled_actual])\n",
    "            scaled_actual = scaler.fit_transform(np.array(scaled_actual[:, np.newaxis]))\n",
    "            scaled_preds = scaler.transform([preds])\n",
    "            scaled_preds = scaled_preds.reshape(-1, 1)\n",
    "            mse = mean_squared_error(scaled_actual, scaled_preds)\n",
    "            global_loss += mse\n",
    "\n",
    "        return {'loss': global_loss, 'status': STATUS_OK}\n",
    "    \n",
    "    except BaseException as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return {'status': STATUS_FAIL}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    global_objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:158: DeprecationWarning: Trainer argument \"learning_rate_decay_factor\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:166: DeprecationWarning: Trainer argument \"patience\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "/Users/dan/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/trainer/_base.py:172: DeprecationWarning: Trainer argument \"minimum_learning_rate\" is deprecated. Use callbacks instead.\n",
      "  DeprecationWarning,\n",
      "100%|██████████| 50/50 [00:07<00:00,  6.46it/s, epoch=1/1, avg_epoch_loss=0.0997]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-de0e291ffad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgluon_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Skip graphs with absurd loss, stablecoins etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/model/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, dataset, num_samples, num_workers, num_prefetch, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0moutput_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             )\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gluonts/model/forecast_generator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inference_data_loader, prediction_net, input_names, freq, output_transform, num_samples, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minference_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m                             '1 positional argument')\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gluonts/mx/model/predictor.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(prediction_net, inputs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mpredict_to_numpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_net\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprediction_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2564\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2565\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2566\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   2567\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "mx.random.seed(0)\n",
    "np.random.seed(0)\n",
    "trainer = Trainer(epochs=1, learning_rate=0.0001)\n",
    "estimator = deepar.DeepAREstimator(\n",
    "        freq=\"D\", \n",
    "#         num_cells=150,\n",
    "        num_layers=4.99,\n",
    "        prediction_length=prediction_length, \n",
    "        trainer=trainer,\n",
    "#         context_length=100,\n",
    "#         use_feat_dynamic_real=True,\n",
    "    )\n",
    "\n",
    "predictor = estimator.train(\n",
    "    training_data=train_data,\n",
    "#     validation_data=validation_data\n",
    ")\n",
    "\n",
    "global_loss = 0\n",
    "predictions = predictor.predict(train_data.list_data)\n",
    "for index, value in enumerate(range(len(gluon_list))):\n",
    "    prediction = next(predictions)\n",
    "    name = test_data.list_data[index]['name']\n",
    "    # Skip graphs with absurd loss, stablecoins etc.\n",
    "    if name in {\"USDT-USD.csv\", \"CCXX-USD.csv\", \"TUSD-USD.csv\"}:\n",
    "        continue\n",
    "#     print(name)\n",
    "    full_actual = test_data.list_data[index]['target']\n",
    "    actual = full_actual[-30:]\n",
    "    preds = pd.Series(prediction.mean)\n",
    "    preds.index = actual.index\n",
    "    # PLOT ALL CRYPTO PREDICTIONS\n",
    "#     plt.figure()\n",
    "#     preds.plot(legend=True, label=f\"{name} PREDICTED\")\n",
    "#     actual.plot(legend=True, label=f\"{name} ACTUAL\")\n",
    "#     plt.show()\n",
    "\n",
    "    # SCALING\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_actual = np.array(actual)\n",
    "    scaler.fit([scaled_actual])\n",
    "    scaled_actual = scaler.fit_transform(np.array(scaled_actual[:, np.newaxis]))\n",
    "    scaled_preds = scaler.transform([preds])\n",
    "    scaled_preds = scaled_preds.reshape(-1, 1)\n",
    "    mse = mean_squared_error(scaled_actual, scaled_preds)\n",
    "#     print(f\"mse: {mse}\")\n",
    "    global_loss += mse\n",
    "    \n",
    "print(f\"global_loss is {global_loss}\")\n",
    "print(f\"average global_loss is {global_loss / len(gluon_list)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "21 with absolutely nothing\n",
    "Doesn't work with dropna\n",
    "25 with fillna\n",
    "\n",
    "\n",
    "122.9 BASE 20 epochs, learning_rate=0.01, num_cells=50\n",
    "119 BASE 10 epochs, learning_rate=0.01, num_cells=100, num_layers=3\n",
    "91.1 BASE 10 epochs, learning_rate=0.01, num_cells=100\n",
    "83 BASE 20 epochs, learning_rate=0.003\n",
    "78.4 BASE 10 epochs, context_length=60\n",
    "78 150, 3, 0.001, 1 epochs\n",
    "67.07 BASE 10 epochs, context_length=100\n",
    "67.07 BASE 10 epochs, context_length=100, batch_size=64\n",
    "56.76 BASE 20 epochs, learning_rate=0.01\n",
    "55.4 BASE num_cells=150, num_layers=3, 10 epochs, context_length=100, batch_size=64 \n",
    "45.82 BASE 20 epochs, num_cells=60, num_layers=3\n",
    "40.32 BASE 10 epochs\n",
    "37 BASE 1 epochs\n",
    "36.6 BASE 20 epochs, num_cells=50, num_layers=3, learning_rate=0.0001\n",
    "34.75 BASE 10 epochs, num_cells=60\n",
    "33.68 BASE 3 epochs\n",
    "32.56 BASE 20 epochs, num_cells=60\n",
    "30.9 BASE 20 epochs, num_cells=60, num_layers=3, learning_rate=0.0001\n",
    "29.57 BASE 10 epochs, num_cells=50\n",
    "26.7 BASE 2 epochs\n",
    "23.4 BASE 5 epochs\n",
    "22.86 BASE 20 epochs, learning_rate=0.0002\n",
    "21.95 BASE 50 epochs\n",
    "21.95 BASE 30 epochs\n",
    "21.52 BASE 20 epochs, learning_rate=0.0001\n",
    "21.1 BASE 20 epochs, learning_rate=0.0005\n",
    "21 150, 3, 0.001, 50 epochs\n",
    "20.9 BASE 20 epochs\n",
    "\n",
    "\n",
    "32 num_cells=150, num_layers=3, 0.0001, 40 epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__\n"
     ]
    }
   ],
   "source": [
    "print(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
